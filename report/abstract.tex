With current developments in simulation, data science, and machine learning research becomes ever more data-driven and compute-intensive. Coupled with the decreased prices of storage due to the ability of using more heterogeneous, less specialized hardware the usage of data lakes becomes evermore important and viable for research. In order to manage the sheer amount of raw, unprocessed data, a both sufficient and performant metadata management solution is paramount.

After internal performance evaluation of Elasticsearch for data lake metadata management at the GWDG, it was found out that rally, Elastics official benchmarker, does not scale well enough to simulate realistic \ac{HPC} workload. This report developed a \acs{MPI}-based, \acs{HPC}-native benchmarking framework for evaluting Elasticsearchs performance. A distributed, I/O-optimized ingestion benchmarker was designed and implemented, which measures both the latency and write throughput when ingesting large corpora of documents. Furthermore, a distributed query benchmarker was designed, which allows for custom scenarios using a newly developed, JSON-based DSL embedding the Elastcisearch Search API DSL. All syntax is conceptually similar to rally, which makes porting those battle-tested benchmarks simple.

Additionally, a \acs{MPI}-based, zero-configuration, stateful workflow to automatically create and (re-)spawn an Elasticsearch cluster in a dynamic SLURM environment. By dynamically patching the Elasticsearch configuration used in the Singularity containers with the new hostnames assigned by SLURM, the cluster becomes hardware independent and can easily be reused in future jobs. Lastly, a full end-to-end benchmarking workflow was designed.

The benchmarker was tested on the Emmy \acs{HPC} cluster using 3 nodes. For this, Elastics \texttt{nyc\_taxis} benchmark track was successfully ported and extended with further scenarios. It can be seen that the benchmark successfully records the expected scaling behaviour when increasing the number of load generators per cluster node.

Concluding, the benchmarker successfully provides a large-scale alternative to the canonical rally benchmarker, allowing further research in \ac{HPC}-native, high-throughput use case benchmarking for data lake applications.
